# Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¹Ù…Ù„ÛŒ Ø±ÙØ¹ Ù…Ø´Ú©Ù„Ø§Øª - Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ùˆ ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§

## ğŸ¯ Ù‡Ø¯Ù
Ø±ÙØ¹ Ù…Ø´Ú©Ù„Ø§Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ Ø¯Ø± Ø®Ø·ÙˆØ· 13-29 Ø§Ø² `IMPROVEMENT_RECOMMENDATIONS.md` Ø¨Ø§ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ùˆ Ù‚Ø§Ø¨Ù„ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ

---

## ğŸ“‹ Ù…Ø´Ú©Ù„Ø§Øª Ùˆ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ

### Ø¨Ø®Ø´ 1: Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ

#### âŒ Ù…Ø´Ú©Ù„ 1: ÙÙ‚Ø· Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø§Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÚ©Ø±Ø§Ø± Ú©Ù„Ù…Ø§Øª

**ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ:**
```python
# Ø¯Ø± seo_analyzer.py - Ø®Ø· 321
def _extract_keywords(self, text: str) -> List[Dict[str, Any]]:
    # ÙÙ‚Ø· Ø´Ù…Ø§Ø±Ø´ ØªÚ©Ø±Ø§Ø± Ú©Ù„Ù…Ø§Øª
    word_freq = Counter(filtered_words)
    keywords = [{'word': word, 'count': count} for word, count in word_freq.most_common(50)]
```

**Ø±Ø§Ù‡â€ŒØ­Ù„: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Keyword Extractor Ù¾ÛŒØ´Ø±ÙØªÙ‡**

```python
# Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯: backend/core/keyword_research/advanced_keyword_extractor.py

import re
from collections import Counter
from typing import List, Dict, Any
import logging
from keybert import KeyBERT
from rake_nltk import Rake
import yake

logger = logging.getLogger(__name__)


class AdvancedKeywordExtractor:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú†Ù†Ø¯ Ø±ÙˆØ´"""
    
    def __init__(self, language: str = 'fa'):
        self.language = language
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² KeyBERT Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù†Ø§ÛŒÛŒ
        try:
            model_name = 'paraphrase-multilingual-MiniLM-L12-v2' if language == 'fa' else 'all-MiniLM-L6-v2'
            self.keybert = KeyBERT(model=model_name)
        except:
            self.keybert = None
            logger.warning("KeyBERT not available, using fallback methods")
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² RAKE
        self.rake = Rake(language=language if language == 'en' else None)
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² YAKE
        self.yake = yake.KeywordExtractor(
            lan=language,
            n=3,  # Ø­Ø¯Ø§Ú©Ø«Ø± 3 Ú©Ù„Ù…Ù‡
            dedupLim=0.7,
            top=50
        )
    
    def extract_keywords(
        self,
        text: str,
        min_length: int = 2,
        max_length: int = 3,
        top_n: int = 50
    ) -> List[Dict[str, Any]]:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø§ ØªØ±Ú©ÛŒØ¨ Ú†Ù†Ø¯ Ø±ÙˆØ´
        
        Returns:
            Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø§ Ø§Ù…ØªÛŒØ§Ø² Ùˆ Ù…Ù†Ø¨Ø¹
        """
        all_keywords = {}
        
        # Ø±ÙˆØ´ 1: KeyBERT (Ù…Ø¹Ù†Ø§ÛŒÛŒ)
        if self.keybert:
            try:
                keybert_keywords = self.keybert.extract_keywords(
                    text,
                    keyphrase_ngram_range=(1, max_length),
                    stop_words=None,
                    top_n=top_n
                )
                for keyword, score in keybert_keywords:
                    if keyword not in all_keywords:
                        all_keywords[keyword] = {
                            'keyword': keyword,
                            'score': score,
                            'method': 'keybert',
                            'frequency': 0
                        }
            except Exception as e:
                logger.error(f"KeyBERT extraction failed: {str(e)}")
        
        # Ø±ÙˆØ´ 2: RAKE (Rapid Automatic Keyword Extraction)
        try:
            self.rake.extract_keywords_from_text(text)
            rake_keywords = self.rake.get_ranked_phrases()[:top_n]
            for keyword in rake_keywords:
                if len(keyword.split()) <= max_length:
                    if keyword not in all_keywords:
                        all_keywords[keyword] = {
                            'keyword': keyword,
                            'score': 0.5,
                            'method': 'rake',
                            'frequency': 0
                        }
        except Exception as e:
            logger.error(f"RAKE extraction failed: {str(e)}")
        
        # Ø±ÙˆØ´ 3: YAKE
        try:
            yake_keywords = self.yake.extract_keywords(text)
            for score, keyword in yake_keywords:
                if keyword not in all_keywords:
                    all_keywords[keyword] = {
                        'keyword': keyword,
                        'score': 1 - score,  # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ù…Ø«Ø¨Øª
                        'method': 'yake',
                        'frequency': 0
                    }
        except Exception as e:
            logger.error(f"YAKE extraction failed: {str(e)}")
        
        # Ø±ÙˆØ´ 4: Frequency-based (Ø±ÙˆØ´ ÙØ¹Ù„ÛŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† fallback)
        words = re.findall(r'\b\w+\b', text.lower())
        word_freq = Counter(words)
        for word, count in word_freq.most_common(top_n):
            if len(word) >= min_length and word not in all_keywords:
                all_keywords[word] = {
                    'keyword': word,
                    'score': count / max(word_freq.values()) if word_freq else 0,
                    'method': 'frequency',
                    'frequency': count
                }
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² ØªØ±Ú©ÛŒØ¨ÛŒ
        for keyword_data in all_keywords.values():
            keyword_data['combined_score'] = self._calculate_combined_score(keyword_data)
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø² ØªØ±Ú©ÛŒØ¨ÛŒ
        sorted_keywords = sorted(
            all_keywords.values(),
            key=lambda x: x['combined_score'],
            reverse=True
        )
        
        return sorted_keywords[:top_n]
    
    def _calculate_combined_score(self, keyword_data: Dict[str, Any]) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² ØªØ±Ú©ÛŒØ¨ÛŒ"""
        score = keyword_data.get('score', 0)
        frequency = keyword_data.get('frequency', 0)
        method = keyword_data.get('method', '')
        
        # ÙˆØ²Ù†â€ŒØ¯Ù‡ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±ÙˆØ´
        method_weights = {
            'keybert': 1.0,
            'yake': 0.8,
            'rake': 0.7,
            'frequency': 0.5
        }
        
        method_weight = method_weights.get(method, 0.5)
        
        # ØªØ±Ú©ÛŒØ¨ Ø§Ù…ØªÛŒØ§Ø² Ùˆ ØªÚ©Ø±Ø§Ø±
        combined = (score * method_weight) + (min(frequency / 10, 1.0) * 0.3)
        
        return min(combined, 1.0)
```

**ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²:**
- `backend/core/keyword_research/__init__.py`
- `backend/core/keyword_research/advanced_keyword_extractor.py`

**ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯:**
```txt
keybert==0.8.0
rake-nltk==1.0.7
yake==0.4.8
```

---

#### âŒ Ù…Ø´Ú©Ù„ 2: Ø¹Ø¯Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² APIÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ ØªØ­Ù‚ÛŒÙ‚ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ

**Ø±Ø§Ù‡â€ŒØ­Ù„: ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ Google Keyword Planner (Ø±Ø§ÛŒÚ¯Ø§Ù†)**

```python
# Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„: backend/core/keyword_research/google_keyword_planner.py

import os
import logging
import httpx
from typing import Dict, Any, List, Optional
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


class GoogleKeywordPlanner:
    """Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Google Keyword Planner (Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¨Ø§ Google Ads account)"""
    
    def __init__(self):
        # Google Keyword Planner Ù†ÛŒØ§Ø² Ø¨Ù‡ Google Ads API Ø¯Ø§Ø±Ø¯
        # Ø§Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø§Ø² Google Trends Ùˆ Autocomplete Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…
        self.base_url = "https://www.google.com"
    
    async def get_keyword_suggestions(
        self,
        seed_keyword: str,
        language: str = 'fa',
        country: str = 'ir'
    ) -> List[Dict[str, Any]]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Google Autocomplete
        
        Ø§ÛŒÙ† Ø±ÙˆØ´ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø§Ø³Øª Ùˆ Ù†ÛŒØ§Ø² Ø¨Ù‡ API Key Ù†Ø¯Ø§Ø±Ø¯
        """
        suggestions = []
        
        try:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Google Autocomplete API (ØºÛŒØ±Ø±Ø³Ù…ÛŒ Ø§Ù…Ø§ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯)
            url = f"http://suggestqueries.google.com/complete/search"
            params = {
                'client': 'firefox',
                'q': seed_keyword,
                'hl': language,
                'gl': country
            }
            
            async with httpx.AsyncClient() as client:
                response = await client.get(url, params=params, timeout=10.0)
                
                if response.status_code == 200:
                    import json
                    data = json.loads(response.text)
                    if len(data) > 1:
                        suggestions = [
                            {
                                'keyword': keyword,
                                'type': 'autocomplete',
                                'source': 'google'
                            }
                            for keyword in data[1][:10]
                        ]
        except Exception as e:
            logger.error(f"Error fetching Google suggestions: {str(e)}")
        
        return suggestions
    
    async def get_related_searches(
        self,
        keyword: str,
        language: str = 'fa'
    ) -> List[str]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø³ØªØ¬ÙˆÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø§Ø² Google
        (Ø§Ø² Ø·Ø±ÛŒÙ‚ scraping ØµÙØ­Ù‡ Ù†ØªØ§ÛŒØ¬)
        """
        related = []
        
        try:
            url = f"https://www.google.com/search"
            params = {
                'q': keyword,
                'hl': language
            }
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with httpx.AsyncClient() as client:
                response = await client.get(url, params=params, headers=headers, timeout=10.0)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¨Ø®Ø´ "People also ask"
                    related_sections = soup.find_all('div', class_='related-question-pair')
                    for section in related_sections[:5]:
                        text = section.get_text(strip=True)
                        if text:
                            related.append(text)
        except Exception as e:
            logger.error(f"Error fetching related searches: {str(e)}")
        
        return related
```

---

#### âŒ Ù…Ø´Ú©Ù„ 3: Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ ØªØ­Ù„ÛŒÙ„ Ø³Ø®ØªÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ (Keyword Difficulty)

**Ø±Ø§Ù‡â€ŒØ­Ù„: Ù…Ø­Ø§Ø³Ø¨Ù‡ Keyword Difficulty Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ API**

```python
# Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„: backend/core/keyword_research/keyword_difficulty.py

import httpx
import logging
from typing import Dict, Any
from bs4 import BeautifulSoup
import re

logger = logging.getLogger(__name__)


class KeywordDifficultyCalculator:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Keyword Difficulty Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ API Ù¾ÙˆÙ„ÛŒ"""
    
    async def calculate_difficulty(
        self,
        keyword: str,
        language: str = 'fa'
    ) -> Dict[str, Any]:
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Keyword Difficulty Ø¨Ø± Ø§Ø³Ø§Ø³:
        1. ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ
        2. Domain Authority ØµÙØ­Ø§Øª Ø±ØªØ¨Ù‡â€ŒØ¯Ø§Ø± (ØªÙ‚Ø±ÛŒØ¨ÛŒ)
        3. Ú©ÛŒÙÛŒØª Ù…Ø­ØªÙˆØ§ÛŒ Ø±Ù‚Ø¨Ø§
        4. Ø³Ù† Ø¯Ø§Ù…Ù†Ù‡ (ØªÙ‚Ø±ÛŒØ¨ÛŒ)
        """
        try:
            # Ø¯Ø±ÛŒØ§ÙØª Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ
            search_results = await self._get_search_results(keyword, language)
            
            if not search_results:
                return {
                    'difficulty': 50,
                    'level': 'medium',
                    'confidence': 'low'
                }
            
            # ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬
            total_results = search_results.get('total_results', 0)
            top_domains = search_results.get('top_domains', [])
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Difficulty
            difficulty_score = 0
            
            # ÙØ§Ú©ØªÙˆØ± 1: ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬ (Ù‡Ø±Ú†Ù‡ Ø¨ÛŒØ´ØªØ±ØŒ Ø³Ø®Øªâ€ŒØªØ±)
            if total_results > 10000000:
                difficulty_score += 40
            elif total_results > 1000000:
                difficulty_score += 30
            elif total_results > 100000:
                difficulty_score += 20
            else:
                difficulty_score += 10
            
            # ÙØ§Ú©ØªÙˆØ± 2: Ù‚Ø¯Ø±Øª Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø±ØªØ¨Ù‡â€ŒØ¯Ø§Ø±
            strong_domains = ['wikipedia.org', 'youtube.com', 'amazon.com', 'facebook.com']
            strong_count = sum(1 for domain in top_domains if any(sd in domain for sd in strong_domains))
            
            if strong_count >= 3:
                difficulty_score += 40
            elif strong_count >= 2:
                difficulty_score += 30
            elif strong_count >= 1:
                difficulty_score += 20
            else:
                difficulty_score += 10
            
            # ÙØ§Ú©ØªÙˆØ± 3: Ø·ÙˆÙ„ Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ (Long-tail Ø¢Ø³Ø§Ù†â€ŒØªØ± Ø§Ø³Øª)
            keyword_length = len(keyword.split())
            if keyword_length >= 4:
                difficulty_score -= 20
            elif keyword_length >= 3:
                difficulty_score -= 10
            
            # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ 0-100
            difficulty_score = max(0, min(100, difficulty_score))
            
            # ØªØ¹ÛŒÛŒÙ† Ø³Ø·Ø­
            if difficulty_score >= 70:
                level = 'hard'
            elif difficulty_score >= 40:
                level = 'medium'
            else:
                level = 'easy'
            
            return {
                'difficulty': difficulty_score,
                'level': level,
                'total_results': total_results,
                'top_domains': top_domains[:5],
                'confidence': 'medium'
            }
            
        except Exception as e:
            logger.error(f"Error calculating keyword difficulty: {str(e)}")
            return {
                'difficulty': 50,
                'level': 'medium',
                'confidence': 'low',
                'error': str(e)
            }
    
    async def _get_search_results(
        self,
        keyword: str,
        language: str = 'fa'
    ) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ Ø§Ø² Google"""
        try:
            url = "https://www.google.com/search"
            params = {
                'q': keyword,
                'hl': language,
                'num': 10
            }
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with httpx.AsyncClient() as client:
                response = await client.get(url, params=params, headers=headers, timeout=10.0)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù†ØªØ§ÛŒØ¬
                    result_stats = soup.find('div', {'id': 'result-stats'})
                    total_results = 0
                    if result_stats:
                        text = result_stats.get_text()
                        numbers = re.findall(r'[\d,]+', text.replace(',', ''))
                        if numbers:
                            total_results = int(numbers[0].replace(',', ''))
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø±ØªØ¨Ù‡â€ŒØ¯Ø§Ø±
                    top_domains = []
                    search_results = soup.find_all('div', class_='g')[:10]
                    for result in search_results:
                        link = result.find('a', href=True)
                        if link:
                            href = link['href']
                            if 'http' in href:
                                from urllib.parse import urlparse
                                domain = urlparse(href).netloc
                                if domain and domain not in top_domains:
                                    top_domains.append(domain)
                    
                    return {
                        'total_results': total_results,
                        'top_domains': top_domains
                    }
        except Exception as e:
            logger.error(f"Error getting search results: {str(e)}")
        
        return {}
```

---

#### âŒ Ù…Ø´Ú©Ù„ 4: Ø¹Ø¯Ù… Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø­Ø¬Ù… Ø¬Ø³ØªØ¬Ùˆ (Search Volume)

**Ø±Ø§Ù‡â€ŒØ­Ù„: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Google Trends (Ø±Ø§ÛŒÚ¯Ø§Ù†)**

```python
# Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„: backend/core/keyword_research/google_trends.py

import httpx
import logging
from typing import Dict, Any, List
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


class GoogleTrendsAnalyzer:
    """ØªØ­Ù„ÛŒÙ„ Google Trends Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø­Ø¬Ù… Ø¬Ø³ØªØ¬Ùˆ (ØªÙ‚Ø±ÛŒØ¨ÛŒ)"""
    
    async def get_trend_data(
        self,
        keyword: str,
        timeframe: str = '12m'  # 12 Ù…Ø§Ù‡ Ú¯Ø°Ø´ØªÙ‡
    ) -> Dict[str, Any]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Trend Ø§Ø² Google Trends
        
        Returns:
            {
                'average_volume': int,  # Ø­Ø¬Ù… Ù…ØªÙˆØ³Ø· (Ù†Ø³Ø¨ÛŒ)
                'trend': List[int],    # Ø±ÙˆÙ†Ø¯ 12 Ù…Ø§Ù‡Ù‡
                'peak_month': str,      # Ù…Ø§Ù‡ Ù¾ÛŒÚ©
                'growth_rate': float    # Ù†Ø±Ø® Ø±Ø´Ø¯
            }
        """
        try:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² pytrends (Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Python)
            from pytrends.request import TrendReq
            
            pytrends = TrendReq(hl='fa-IR', tz=360)
            pytrends.build_payload([keyword], timeframe=timeframe)
            
            # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ trend
            trend_data = pytrends.interest_over_time()
            
            if not trend_data.empty:
                values = trend_data[keyword].tolist()
                avg_volume = sum(values) / len(values) if values else 0
                
                # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø§Ù‡ Ù¾ÛŒÚ©
                peak_index = values.index(max(values))
                peak_month = trend_data.index[peak_index].strftime('%Y-%m')
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ø±Ø® Ø±Ø´Ø¯
                if len(values) >= 2:
                    growth_rate = ((values[-1] - values[0]) / values[0] * 100) if values[0] > 0 else 0
                else:
                    growth_rate = 0
                
                return {
                    'average_volume': int(avg_volume),
                    'trend': values,
                    'peak_month': peak_month,
                    'growth_rate': round(growth_rate, 2),
                    'relative_volume': self._estimate_absolute_volume(avg_volume)
                }
        except Exception as e:
            logger.error(f"Error getting Google Trends data: {str(e)}")
        
        return {
            'average_volume': 0,
            'trend': [],
            'peak_month': None,
            'growth_rate': 0,
            'relative_volume': 'unknown'
        }
    
    def _estimate_absolute_volume(self, relative_volume: float) -> str:
        """ØªØ®Ù…ÛŒÙ† Ø­Ø¬Ù… Ù…Ø·Ù„Ù‚ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø­Ø¬Ù… Ù†Ø³Ø¨ÛŒ"""
        if relative_volume >= 80:
            return 'very_high'  # 100K+
        elif relative_volume >= 50:
            return 'high'  # 10K-100K
        elif relative_volume >= 20:
            return 'medium'  # 1K-10K
        elif relative_volume >= 5:
            return 'low'  # 100-1K
        else:
            return 'very_low'  # <100
```

**ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø¬Ø¯ÛŒØ¯:**
```txt
pytrends==4.9.2
```

---

#### âŒ Ù…Ø´Ú©Ù„ 5: Ø¹Ø¯Ù… Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Long-tail

**Ø±Ø§Ù‡â€ŒØ­Ù„: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Long-tail Keywords**

```python
# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡: backend/core/keyword_research/advanced_keyword_extractor.py

class LongTailKeywordExtractor:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Long-tail"""
    
    def __init__(self):
        self.google_planner = GoogleKeywordPlanner()
    
    async def extract_long_tail_keywords(
        self,
        seed_keywords: List[str],
        min_length: int = 4,  # Ø­Ø¯Ø§Ù‚Ù„ 4 Ú©Ù„Ù…Ù‡
        max_results: int = 50
    ) -> List[Dict[str, Any]]:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Long-tail
        
        Ø±ÙˆØ´â€ŒÙ‡Ø§:
        1. Google Autocomplete
        2. People Also Ask
        3. Related Searches
        4. ØªØ±Ú©ÛŒØ¨ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
        """
        all_long_tail = []
        
        for seed_keyword in seed_keywords:
            # Ø±ÙˆØ´ 1: Google Autocomplete
            suggestions = await self.google_planner.get_keyword_suggestions(seed_keyword)
            for suggestion in suggestions:
                keyword = suggestion['keyword']
                if len(keyword.split()) >= min_length:
                    all_long_tail.append({
                        'keyword': keyword,
                        'type': 'autocomplete',
                        'seed': seed_keyword
                    })
            
            # Ø±ÙˆØ´ 2: Related Searches
            related = await self.google_planner.get_related_searches(seed_keyword)
            for rel_keyword in related:
                if len(rel_keyword.split()) >= min_length:
                    all_long_tail.append({
                        'keyword': rel_keyword,
                        'type': 'related',
                        'seed': seed_keyword
                    })
            
            # Ø±ÙˆØ´ 3: ØªØ±Ú©ÛŒØ¨ Ø¨Ø§ Ú©Ù„Ù…Ø§Øª Ø§Ø¶Ø§ÙÛŒ
            modifiers = ['Ú†Ú¯ÙˆÙ†Ù‡', 'Ø¨Ù‡ØªØ±ÛŒÙ†', 'Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ', 'Ø¢Ù…ÙˆØ²Ø´', 'Ù…Ù‚Ø§ÛŒØ³Ù‡']
            for modifier in modifiers:
                long_tail = f"{modifier} {seed_keyword}"
                all_long_tail.append({
                    'keyword': long_tail,
                    'type': 'combination',
                    'seed': seed_keyword
                })
        
        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
        unique_keywords = {}
        for item in all_long_tail:
            keyword = item['keyword'].lower().strip()
            if keyword not in unique_keywords:
                unique_keywords[keyword] = item
        
        return list(unique_keywords.values())[:max_results]
```

---

### Ø¨Ø®Ø´ 2: ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§

#### âŒ Ù…Ø´Ú©Ù„ 6: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø«Ø§Ø¨Øª (Template-based)

**Ø±Ø§Ù‡â€ŒØ­Ù„: ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ OpenAI (Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± requirements.txt)**

```python
# Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: backend/core/content_generator.py

# Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ÛŒ ÙØ§ÛŒÙ„ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯:
import os
from openai import AsyncOpenAI

class ContentGenerator:
    def __init__(self):
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† AI Generator
        self.openai_client = None
        if os.getenv('OPENAI_API_KEY'):
            try:
                self.openai_client = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'))
                logger.info("OpenAI client initialized")
            except Exception as e:
                logger.warning(f"OpenAI not available: {str(e)}")
    
    async def _generate_text_content_for_keyword(
        self,
        keyword: str,
        site_url: str,
        language: str = 'fa'
    ) -> List[Dict[str, Any]]:
        """ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ - Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§ AIØŒ Ø³Ù¾Ø³ Template"""
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² AI Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
        if self.openai_client:
            try:
                return await self._generate_with_ai(keyword, language)
            except Exception as e:
                logger.warning(f"AI generation failed, using template: {str(e)}")
        
        # Fallback Ø¨Ù‡ Template
        return await self._generate_template_content(keyword, site_url, language)
    
    async def _generate_with_ai(
        self,
        keyword: str,
        language: str = 'fa'
    ) -> List[Dict[str, Any]]:
        """ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ Ø¨Ø§ OpenAI"""
        
        system_prompt = "You are an expert SEO content writer." if language == 'en' else "Ø´Ù…Ø§ ÛŒÚ© Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ù…ØªØ®ØµØµ SEO Ù‡Ø³ØªÛŒØ¯."
        
        user_prompt = f"""
        Write a comprehensive, SEO-optimized article about "{keyword}".
        
        Requirements:
        - Language: {language}
        - Length: 1500-2000 words
        - Use keyword naturally (1-2% density)
        - Well-structured with H2 and H3 headings
        - Valuable and engaging content
        - Include introduction and conclusion
        
        Write the article now:
        """ if language == 'en' else f"""
        ÛŒÚ© Ù…Ù‚Ø§Ù„Ù‡ Ø¬Ø§Ù…Ø¹ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ SEO Ø¯Ø±Ø¨Ø§Ø±Ù‡ "{keyword}" Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.
        
        Ø§Ù„Ø²Ø§Ù…Ø§Øª:
        - Ø²Ø¨Ø§Ù†: ÙØ§Ø±Ø³ÛŒ
        - Ø·ÙˆÙ„: 1500-2000 Ú©Ù„Ù…Ù‡
        - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø·Ø¨ÛŒØ¹ÛŒ Ø§Ø² Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ (Ú†Ú¯Ø§Ù„ÛŒ 1-2%)
        - Ø³Ø§Ø®ØªØ§Ø± Ù…Ù†Ø¸Ù… Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù†â€ŒÙ‡Ø§ÛŒ H2 Ùˆ H3
        - Ù…Ø­ØªÙˆØ§ÛŒ Ø§Ø±Ø²Ø´Ù…Ù†Ø¯ Ùˆ Ø¬Ø°Ø§Ø¨
        - Ø´Ø§Ù…Ù„ Ù…Ù‚Ø¯Ù…Ù‡ Ùˆ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ
        
        Ù…Ù‚Ø§Ù„Ù‡ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯:
        """
        
        response = await self.openai_client.chat.completions.create(
            model=os.getenv('AI_CONTENT_MODEL', 'gpt-3.5-turbo'),
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7,
            max_tokens=2000
        )
        
        content = response.choices[0].message.content
        
        return [{
            'id': f"content_{hash(keyword)}_{datetime.now().timestamp()}",
            'title': self._extract_title(content),
            'content': content,
            'type': 'article',
            'word_count': len(content.split()),
            'keywords': [keyword],
            'status': 'generated',
            'seo_score': 85,
            'created_at': datetime.now().isoformat(),
            'generated_by': 'openai'
        }]
```

---

## ğŸ“¦ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯

Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ù‡ `backend/requirements.txt`:

```txt
# Keyword Research
keybert==0.8.0
rake-nltk==1.0.7
yake==0.4.8
pytrends==4.9.2

# OpenAI (Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª Ø§Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯)
openai==1.3.5
```

---

## ğŸš€ Ù…Ø±Ø§Ø­Ù„ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ

### Ù…Ø±Ø­Ù„Ù‡ 1: Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§
```bash
cd backend
pip install keybert rake-nltk yake pytrends
```

### Ù…Ø±Ø­Ù„Ù‡ 2: Ø§ÛŒØ¬Ø§Ø¯ Ø³Ø§Ø®ØªØ§Ø± Ù¾ÙˆØ´Ù‡
```bash
mkdir -p backend/core/keyword_research
touch backend/core/keyword_research/__init__.py
```

### Ù…Ø±Ø­Ù„Ù‡ 3: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
1. `advanced_keyword_extractor.py`
2. `google_keyword_planner.py`
3. `keyword_difficulty.py`
4. `google_trends.py`

### Ù…Ø±Ø­Ù„Ù‡ 4: Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ SEOAnalyzer
```python
# Ø¯Ø± seo_analyzer.py
from .keyword_research.advanced_keyword_extractor import AdvancedKeywordExtractor

class SEOAnalyzer:
    def __init__(self):
        # ...
        self.keyword_extractor = AdvancedKeywordExtractor(language='fa')
    
    def _extract_keywords(self, text: str) -> List[Dict[str, Any]]:
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² AdvancedKeywordExtractor
        return self.keyword_extractor.extract_keywords(text, top_n=50)
```

### Ù…Ø±Ø­Ù„Ù‡ 5: Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ContentGenerator
- Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² OpenAI (Ú©Ø¯ Ø¨Ø§Ù„Ø§)

---

## âœ… Ú†Ú©â€ŒÙ„ÛŒØ³Øª Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ

- [ ] Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
- [ ] Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ `keyword_research`
- [ ] Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ `AdvancedKeywordExtractor`
- [ ] Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ `GoogleKeywordPlanner`
- [ ] Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ `KeywordDifficultyCalculator`
- [ ] Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ `GoogleTrendsAnalyzer`
- [ ] Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ `LongTailKeywordExtractor`
- [ ] Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ `SEOAnalyzer`
- [ ] Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ `ContentGenerator` Ø¨Ø§ OpenAI
- [ ] ØªØ³Øª ØªÙ…Ø§Ù… Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§
- [ ] ØªÙ†Ø¸ÛŒÙ… Environment Variables

---

**Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ!** ğŸš€

