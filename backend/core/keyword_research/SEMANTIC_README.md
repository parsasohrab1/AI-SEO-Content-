# Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Semantic Keyword Analyzer

## ğŸ“‹ Ù…Ø¹Ø±ÙÛŒ

Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„ ØªØ­Ù„ÛŒÙ„ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Word Embeddings Ùˆ NLPØŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ù…Ø±ØªØ¨Ø·ØŒ Ù‡Ù…â€ŒÙ…Ø¹Ù†Ø§Ù‡Ø§ Ùˆ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ LSI Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

## âœ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§

- âœ… Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ù…Ø±ØªØ¨Ø·
- âœ… Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù‡Ù…â€ŒÙ…Ø¹Ù†Ø§Ù‡Ø§ (Synonyms)
- âœ… Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ LSI (Latent Semantic Indexing)
- âœ… Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¹Ù†Ø§
- âœ… Ú¯Ø³ØªØ±Ø´ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ø¹Ù†Ø§ÛŒÛŒ
- âœ… Ø¨Ø±Ø±Ø³ÛŒ Ø±Ø§Ø¨Ø·Ù‡ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
- âœ… Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù† (ÙØ§Ø±Ø³ÛŒ Ùˆ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)

## ğŸš€ Ù†ØµØ¨

### ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²

```bash
pip install sentence-transformers scikit-learn numpy
```

ÛŒØ§ Ø§Ø² requirements.txt:
```bash
pip install -r requirements.txt
```

**Ù†Ú©ØªÙ‡:** Ù…Ø¯Ù„ SentenceTransformer Ø¯Ø± Ø§ÙˆÙ„ÛŒÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (~420 MB).

## ğŸ“– Ø§Ø³ØªÙØ§Ø¯Ù‡

### Ù…Ø«Ø§Ù„ 1: Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ

```python
from backend.core.keyword_research import SemanticKeywordAnalyzer

analyzer = SemanticKeywordAnalyzer()

semantic_keywords = await analyzer.find_semantic_keywords(
    main_keyword="seo optimization",
    threshold=0.7,  # Ø­Ø¯Ø§Ù‚Ù„ similarity
    top_n=20,
    language='en'
)

for kw in semantic_keywords:
    print(f"{kw['keyword']} - similarity: {kw['similarity']:.2f}")
```

### Ù…Ø«Ø§Ù„ 2: Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ LSI

```python
main_keyword = "seo"
context_keywords = [
    "search engine optimization",
    "keyword research",
    "on-page seo",
    "link building"
]

lsi_keywords = await analyzer.find_lsi_keywords(
    main_keyword=main_keyword,
    context_keywords=context_keywords,
    top_n=10
)
```

### Ù…Ø«Ø§Ù„ 3: Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ

```python
keywords = [
    "seo optimization",
    "keyword research",
    "link building",
    "content marketing",
    "social media marketing"
]

clusters = await analyzer.cluster_semantic_keywords(
    keywords=keywords,
    n_clusters=3
)

for cluster_id, cluster_keywords in clusters.items():
    print(f"Ø®ÙˆØ´Ù‡ {cluster_id}: {cluster_keywords}")
```

### Ù…Ø«Ø§Ù„ 4: Ú¯Ø³ØªØ±Ø´ Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ

```python
# Ú¯Ø³ØªØ±Ø´ Ø¨Ù‡ ØµÙˆØ±Øª synonyms
synonyms = await analyzer.expand_keyword_semantically(
    keyword="seo",
    expansion_type='synonyms',
    language='en'
)

# Ú¯Ø³ØªØ±Ø´ Ø¨Ù‡ ØµÙˆØ±Øª related
related = await analyzer.expand_keyword_semantically(
    keyword="seo",
    expansion_type='related',
    language='en'
)
```

### Ù…Ø«Ø§Ù„ 5: Ø¨Ø±Ø±Ø³ÛŒ Ø±Ø§Ø¨Ø·Ù‡ Ù…Ø¹Ù†Ø§ÛŒÛŒ

```python
relationship = analyzer.get_semantic_relationship(
    keyword1="seo",
    keyword2="search engine optimization"
)

print(f"Similarity: {relationship['similarity']:.2f}")
print(f"Relationship: {relationship['relationship']}")
```

## ğŸ“Š Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ

### find_semantic_keywords()

```python
[
    {
        'keyword': str,
        'similarity': float,           # 0-1
        'semantic_relation': str,       # synonym, highly_related, related, lsi
        'source': 'semantic_analysis'
    },
    ...
]
```

### find_lsi_keywords()

```python
[
    {
        'keyword': str,
        'lsi_score': float,            # 0-1
        'context_similarity': float,    # 0-1
        'source': 'lsi_analysis'
    },
    ...
]
```

### cluster_semantic_keywords()

```python
{
    0: ['keyword1', 'keyword2', ...],
    1: ['keyword3', 'keyword4', ...],
    ...
}
```

### get_semantic_relationship()

```python
{
    'similarity': float,        # 0-1
    'relationship': str,        # synonym, highly_related, related, unrelated
    'confidence': float         # 0-1
}
```

## ğŸ¯ Ø§Ù†ÙˆØ§Ø¹ Ø±ÙˆØ§Ø¨Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ

### Synonym (Ù‡Ù…â€ŒÙ…Ø¹Ù†Ø§)
- Similarity: â‰¥ 0.9
- Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ú©Ù‡ Ù…Ø¹Ù†ÛŒ ÛŒÚ©Ø³Ø§Ù†ÛŒ Ø¯Ø§Ø±Ù†Ø¯
- Ù…Ø«Ø§Ù„: "seo" Ùˆ "search engine optimization"

### Highly Related (Ø¨Ø³ÛŒØ§Ø± Ù…Ø±ØªØ¨Ø·)
- Similarity: 0.75 - 0.9
- Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ú©Ù‡ Ø¨Ø³ÛŒØ§Ø± Ù…Ø±ØªØ¨Ø· Ù‡Ø³ØªÙ†Ø¯
- Ù…Ø«Ø§Ù„: "seo" Ùˆ "keyword research"

### Related (Ù…Ø±ØªØ¨Ø·)
- Similarity: 0.6 - 0.75
- Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ú©Ù‡ Ù…Ø±ØªØ¨Ø· Ù‡Ø³ØªÙ†Ø¯
- Ù…Ø«Ø§Ù„: "seo" Ùˆ "content marketing"

### LSI (Latent Semantic Indexing)
- Similarity: 0.5 - 0.6
- Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ú©Ù‡ Ø¯Ø± Ù‡Ù…Ø§Ù† Ø²Ù…ÛŒÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
- Ù…Ø«Ø§Ù„: "seo" Ùˆ "website traffic"

## ğŸ”§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª

### ØªØºÛŒÛŒØ± Ù…Ø¯Ù„

```python
# Ø¯Ø± environment variables
SEMANTIC_MODEL_NAME=paraphrase-multilingual-MiniLM-L12-v2

# ÛŒØ§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±:
# - paraphrase-MiniLM-L6-v2 (Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒØŒ Ø³Ø±ÛŒØ¹â€ŒØªØ±)
# - distiluse-base-multilingual-cased (Ú†Ù†Ø¯Ø²Ø¨Ø§Ù†Ù‡)
```

### Threshold

```python
# threshold Ø¨Ø§Ù„Ø§ØªØ± = Ù†ØªØ§ÛŒØ¬ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø§Ù…Ø§ Ú©Ù…ØªØ±
semantic_keywords = await analyzer.find_semantic_keywords(
    main_keyword="seo",
    threshold=0.8  # ÙÙ‚Ø· Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø§ similarity â‰¥ 0.8
)
```

## ğŸ“ Ù…Ø«Ø§Ù„ Ú©Ø§Ù…Ù„

```python
import asyncio
from backend.core.keyword_research import SemanticKeywordAnalyzer

async def main():
    analyzer = SemanticKeywordAnalyzer()
    
    if not analyzer.model_loaded:
        print("âš ï¸ Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª")
        return
    
    # 1. Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ
    semantic_keywords = await analyzer.find_semantic_keywords(
        main_keyword="seo",
        threshold=0.6,
        top_n=20
    )
    
    # 2. Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
    keywords_list = [kw['keyword'] for kw in semantic_keywords]
    clusters = await analyzer.cluster_semantic_keywords(
        keywords=keywords_list,
        n_clusters=3
    )
    
    # 3. Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
    print(f"âœ… {len(semantic_keywords)} Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ")
    print(f"âœ… {len(clusters)} Ø®ÙˆØ´Ù‡ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")

asyncio.run(main())
```

## ğŸ¯ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§

### 1. Ø¨Ù‡Ø¨ÙˆØ¯ Ù…Ø­ØªÙˆØ§
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¯Ø± Ù…Ø­ØªÙˆØ§
- Ø¨Ù‡Ø¨ÙˆØ¯ Relevance Ø¨Ø±Ø§ÛŒ Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ

### 2. Keyword Research
- Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø±ØªØ¨Ø·
- Ú¯Ø³ØªØ±Ø´ Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ

### 3. Content Optimization
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LSI keywords Ø¯Ø± Ù…Ø­ØªÙˆØ§
- Ø¨Ù‡Ø¨ÙˆØ¯ Semantic SEO

### 4. Keyword Clustering
- Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
- Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù…Ø­ØªÙˆØ§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§

## âš ï¸ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§

### Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù…Ø¯Ù„
- Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ (Ø§ÙˆÙ„ÛŒÙ† Ø¨Ø§Ø± ~420 MB)
- Ù†ÛŒØ§Ø² Ø¨Ù‡ RAM Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„

### Ø¯Ù‚Øª
- Ù†ØªØ§ÛŒØ¬ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¯Ù„ Ù‡Ø³ØªÙ†Ø¯
- Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ Ø¯Ù‚Øª Ú©Ù…ØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯

### Performance
- Ù…Ø­Ø§Ø³Ø¨Ù‡ embeddings Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø²Ù…Ø§Ù†â€ŒØ¨Ø± Ø¨Ø§Ø´Ø¯
- Ø¨Ø±Ø§ÛŒ Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ØŒ Ø§Ø² batch processing Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯

## ğŸ”§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ

### Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Cache

```python
# Cache Ú©Ø±Ø¯Ù† embeddings
import redis
cache = redis.Redis()

async def get_cached_embedding(keyword):
    cache_key = f"embedding:{keyword}"
    cached = cache.get(cache_key)
    if cached:
        return pickle.loads(cached)
    
    embedding = analyzer.model.encode([keyword])[0]
    cache.setex(cache_key, 86400, pickle.dumps(embedding))  # 24 Ø³Ø§Ø¹Øª
    return embedding
```

### Batch Processing

```python
# Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯
keywords = ["seo", "keyword research", "link building"]
embeddings = analyzer.model.encode(keywords, batch_size=32)
```

## ğŸ“š Ù…Ù†Ø§Ø¨Ø¹

- [Sentence Transformers](https://www.sbert.net/)
- [Word Embeddings](https://en.wikipedia.org/wiki/Word_embedding)
- [LSI Keywords](https://www.searchenginejournal.com/lsi-keywords/)
- [Semantic SEO](https://ahrefs.com/blog/semantic-seo/)

---

**Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡:** AI-SEO-Content Team  
**ØªØ§Ø±ÛŒØ®:** 2024

