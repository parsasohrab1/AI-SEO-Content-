"""
ØªØ­Ù„ÛŒÙ„ ÙØ§ØµÙ„Ù‡ Ù…Ø­ØªÙˆØ§ (Content Gap Analysis)
Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÙˆØ¶ÙˆØ¹Ø§ØªØŒ Ø²ÙˆØ§ÛŒØ§ Ùˆ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø­ØªÙˆØ§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø±Ù‚Ø¨Ø§ Ø§Ù…Ø§ Ù†Ù‡ Ø¯Ø± Ø³Ø§ÛŒØª Ø´Ù…Ø§
"""

import logging
from typing import Dict, Any, List, Optional, Set
from collections import Counter
import re

logger = logging.getLogger(__name__)


class ContentGapAnalyzer:
    """
    Ú©Ù„Ø§Ø³ ØªØ­Ù„ÛŒÙ„ ÙØ§ØµÙ„Ù‡ Ù…Ø­ØªÙˆØ§
    
    ØªØ­Ù„ÛŒÙ„:
    - Ù…ÙˆØ¶ÙˆØ¹Ø§ØªÛŒ Ú©Ù‡ Ø±Ù‚Ø¨Ø§ Ù¾ÙˆØ´Ø´ Ø¯Ø§Ø¯Ù‡â€ŒØ§Ù†Ø¯ Ø§Ù…Ø§ Ø´Ù…Ø§ Ù†Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯
    - Ø²ÙˆØ§ÛŒØ§ÛŒ Ù…Ø®ØªÙ„Ù ÛŒÚ© Ù…ÙˆØ¶ÙˆØ¹
    - Ø¹Ù…Ù‚ Ù…Ø­ØªÙˆØ§
    - Ø§Ù†ÙˆØ§Ø¹ Ù…Ø­ØªÙˆØ§ (Ù…Ù‚Ø§Ù„Ù‡ØŒ ÙˆÛŒØ¯ÛŒÙˆØŒ Ø§ÛŒÙ†ÙÙˆÚ¯Ø±Ø§ÙÛŒÚ©)
    """
    
    def __init__(self):
        self.semantic_analyzer = None
        
        # Ø³Ø¹ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Semantic Analyzer Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒÙ…
        try:
            from ..keyword_research.semantic_analyzer import SemanticKeywordAnalyzer
            self.semantic_analyzer = SemanticKeywordAnalyzer()
            if not self.semantic_analyzer.model_loaded:
                logger.warning("Semantic model not loaded. Content gap analysis will use fallback methods.")
        except Exception as e:
            logger.warning(f"Could not load SemanticKeywordAnalyzer: {str(e)}")
    
    async def analyze_content_gaps(
        self,
        site_content: Dict[str, Any],
        competitor_content: List[Dict[str, Any]],
        language: str = 'fa'
    ) -> Dict[str, Any]:
        """
        ØªØ­Ù„ÛŒÙ„ ÙØ§ØµÙ„Ù‡ Ù…Ø­ØªÙˆØ§
        
        Args:
            site_content: Ù…Ø­ØªÙˆØ§ÛŒ Ø³Ø§ÛŒØª Ø´Ù…Ø§
                {
                    'articles': List[Dict],  # Ù…Ù‚Ø§Ù„Ø§Øª
                    'topics': List[str],      # Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
                    'content_types': List[str]  # Ø§Ù†ÙˆØ§Ø¹ Ù…Ø­ØªÙˆØ§
                }
            competitor_content: Ù„ÛŒØ³Øª Ù…Ø­ØªÙˆØ§ÛŒ Ø±Ù‚Ø¨Ø§
                [
                    {
                        'title': str,
                        'content': str,
                        'url': str,
                        'content_type': str,  # article, video, infographic
                        'topics': List[str],
                        'word_count': int
                    },
                    ...
                ]
            language: Ø²Ø¨Ø§Ù†
        
        Returns:
            {
                'topic_gaps': List[Dict],      # Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø±Ù‚Ø¨Ø§ Ø§Ù…Ø§ Ù†Ù‡ Ø¯Ø± Ø´Ù…Ø§
                'angle_gaps': List[Dict],       # Ø²ÙˆØ§ÛŒØ§ÛŒ Ù…Ø®ØªÙ„Ù ÛŒÚ© Ù…ÙˆØ¶ÙˆØ¹
                'depth_gaps': List[Dict],       # ØªÙØ§ÙˆØª Ø¹Ù…Ù‚ Ù…Ø­ØªÙˆØ§
                'content_type_gaps': List[Dict], # Ø§Ù†ÙˆØ§Ø¹ Ù…Ø­ØªÙˆØ§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø±Ù‚Ø¨Ø§
                'recommendations': List[str],   # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª
                'summary': Dict
            }
        """
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
            your_topics = self._extract_topics(site_content, language)
            competitor_topics = self._extract_competitor_topics(competitor_content, language)
            
            # ØªØ­Ù„ÛŒÙ„ ÙØ§ØµÙ„Ù‡ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
            topic_gaps = self._analyze_topic_gaps(your_topics, competitor_topics, language)
            
            # ØªØ­Ù„ÛŒÙ„ Ø²ÙˆØ§ÛŒØ§
            angle_gaps = self._analyze_angle_gaps(site_content, competitor_content, language)
            
            # ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù‚
            depth_gaps = self._analyze_depth_gaps(site_content, competitor_content, language)
            
            # ØªØ­Ù„ÛŒÙ„ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø­ØªÙˆØ§
            content_type_gaps = self._analyze_content_type_gaps(site_content, competitor_content)
            
            # ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª
            recommendations = self._generate_recommendations(
                topic_gaps,
                angle_gaps,
                depth_gaps,
                content_type_gaps,
                language
            )
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®Ù„Ø§ØµÙ‡
            summary = self._calculate_summary(
                topic_gaps,
                angle_gaps,
                depth_gaps,
                content_type_gaps
            )
            
            return {
                'topic_gaps': topic_gaps,
                'angle_gaps': angle_gaps,
                'depth_gaps': depth_gaps,
                'content_type_gaps': content_type_gaps,
                'recommendations': recommendations,
                'summary': summary
            }
            
        except Exception as e:
            logger.error(f"Error analyzing content gaps: {str(e)}")
            return self._empty_gap_result()
    
    def _extract_topics(
        self,
        site_content: Dict[str, Any],
        language: str
    ) -> Set[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø§Ø² Ù…Ø­ØªÙˆØ§ÛŒ Ø³Ø§ÛŒØª Ø´Ù…Ø§"""
        topics = set()
        
        # Ø§Ø² Ù…Ù‚Ø§Ù„Ø§Øª
        articles = site_content.get('articles', [])
        for article in articles:
            article_topics = article.get('topics', [])
            topics.update(article_topics)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² Ø¹Ù†ÙˆØ§Ù†
            title = article.get('title', '')
            if title:
                title_words = self._extract_keywords_from_text(title, language)
                topics.update(title_words)
        
        # Ø§Ø² Ù„ÛŒØ³Øª Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ø³ØªÙ‚ÛŒÙ…
        direct_topics = site_content.get('topics', [])
        topics.update(direct_topics)
        
        return topics
    
    def _extract_competitor_topics(
        self,
        competitor_content: List[Dict[str, Any]],
        language: str
    ) -> Dict[str, List[Dict[str, Any]]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø§Ø² Ù…Ø­ØªÙˆØ§ÛŒ Ø±Ù‚Ø¨Ø§"""
        topic_map = {}
        
        for content_item in competitor_content:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² Ø¹Ù†ÙˆØ§Ù†
            title = content_item.get('title', '')
            title_topics = self._extract_keywords_from_text(title, language)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² Ù…Ø­ØªÙˆØ§
            content_text = content_item.get('content', '')
            content_topics = self._extract_keywords_from_text(content_text[:1000], language)  # 1000 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§ÙˆÙ„
            
            # ØªØ±Ú©ÛŒØ¨ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
            all_topics = set(title_topics) | set(content_topics)
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ø³ØªÙ‚ÛŒÙ…
            direct_topics = content_item.get('topics', [])
            all_topics.update(direct_topics)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± map
            for topic in all_topics:
                if topic not in topic_map:
                    topic_map[topic] = []
                topic_map[topic].append(content_item)
        
        return topic_map
    
    def _extract_keywords_from_text(
        self,
        text: str,
        language: str
    ) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ù…ØªÙ†"""
        if not text:
            return []
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ lowercase
        text_lower = text.lower()
        
        # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ
        text_clean = re.sub(r'[^\w\s]', ' ', text_lower)
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª
        words = text_clean.split()
        
        # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† stop words
        stop_words = {
            'fa': {'Ùˆ', 'Ø¯Ø±', 'Ø¨Ù‡', 'Ø§Ø²', 'Ú©Ù‡', 'Ø§ÛŒÙ†', 'Ø§Ø³Øª', 'Ø±Ø§', 'ÛŒÚ©', 'Ø¢Ù†', 'Ù‡Ø§', 'Ù…ÛŒ', 'Ø´ÙˆØ¯', 'Ø¨Ø±Ø§ÛŒ', 'Ø¨Ø§', 'ØªØ§'},
            'en': {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}
        }
        
        stop_words_set = stop_words.get(language, stop_words['en'])
        filtered_words = [w for w in words if w not in stop_words_set and len(w) > 2]
        
        # Ø´Ù…Ø§Ø±Ø´ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù„Ù…Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø±
        word_freq = Counter(filtered_words)
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù„Ù…Ø§Øª Ø¨Ø§ ØªÚ©Ø±Ø§Ø± >= 2
        keywords = [word for word, count in word_freq.items() if count >= 2]
        
        return keywords[:20]  # Ø­Ø¯Ø§Ú©Ø«Ø± 20 Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ
    
    def _analyze_topic_gaps(
        self,
        your_topics: Set[str],
        competitor_topics: Dict[str, List[Dict[str, Any]]],
        language: str
    ) -> List[Dict[str, Any]]:
        """ØªØ­Ù„ÛŒÙ„ ÙØ§ØµÙ„Ù‡ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª"""
        gaps = []
        
        your_topics_lower = {t.lower() for t in your_topics}
        
        for topic, content_items in competitor_topics.items():
            topic_lower = topic.lower()
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø¯Ø± Ù…Ø­ØªÙˆØ§ÛŒ Ø´Ù…Ø§ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
            if topic_lower not in your_topics_lower:
                # Ø¨Ø±Ø±Ø³ÛŒ similarity Ø¨Ø§ semantic analyzer
                similarity_score = 0.0
                if self.semantic_analyzer and self.semantic_analyzer.model_loaded:
                    # Ø¨Ø±Ø±Ø³ÛŒ similarity Ø¨Ø§ ØªÙ…Ø§Ù… Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø´Ù…Ø§
                    for your_topic in your_topics:
                        try:
                            relationship = self.semantic_analyzer.get_semantic_relationship(
                                topic,
                                your_topic
                            )
                            similarity_score = max(similarity_score, relationship.get('similarity', 0.0))
                        except:
                            pass
                
                # Ø§Ú¯Ø± similarity Ú©Ù… Ø¨Ø§Ø´Ø¯ØŒ ÛŒÚ© gap Ø§Ø³Øª
                if similarity_score < 0.6:
                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù‡Ù…ÛŒØª
                    importance = self._calculate_topic_importance(topic, content_items)
                    
                    gaps.append({
                        'topic': topic,
                        'importance': importance,
                        'competitor_count': len(content_items),
                        'content_items': content_items[:5],  # 5 Ù…ÙˆØ±Ø¯ Ø§ÙˆÙ„
                        'similarity_score': similarity_score,
                        'gap_type': 'topic'
                    })
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù‡Ù…ÛŒØª
        gaps.sort(key=lambda x: x['importance'], reverse=True)
        
        return gaps[:50]  # 50 gap Ø¨Ø±ØªØ±
    
    def _calculate_topic_importance(
        self,
        topic: str,
        content_items: List[Dict[str, Any]]
    ) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù‡Ù…ÛŒØª Ù…ÙˆØ¶ÙˆØ¹"""
        score = 0.0
        
        # ÙØ§Ú©ØªÙˆØ± 1: ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­ØªÙˆØ§ (40%)
        count = len(content_items)
        if count >= 5:
            score += 40
        elif count >= 3:
            score += 30
        elif count >= 2:
            score += 20
        else:
            score += 10
        
        # ÙØ§Ú©ØªÙˆØ± 2: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø·ÙˆÙ„ Ù…Ø­ØªÙˆØ§ (30%)
        avg_length = sum(
            item.get('word_count', 0) for item in content_items
        ) / len(content_items) if content_items else 0
        
        if avg_length >= 2000:
            score += 30
        elif avg_length >= 1500:
            score += 25
        elif avg_length >= 1000:
            score += 20
        else:
            score += 10
        
        # ÙØ§Ú©ØªÙˆØ± 3: ØªÙ†ÙˆØ¹ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø­ØªÙˆØ§ (30%)
        content_types = set(item.get('content_type', 'article') for item in content_items)
        type_count = len(content_types)
        
        if type_count >= 3:
            score += 30
        elif type_count >= 2:
            score += 20
        else:
            score += 10
        
        return min(score, 100.0)
    
    def _analyze_angle_gaps(
        self,
        site_content: Dict[str, Any],
        competitor_content: List[Dict[str, Any]],
        language: str
    ) -> List[Dict[str, Any]]:
        """ØªØ­Ù„ÛŒÙ„ ÙØ§ØµÙ„Ù‡ Ø²ÙˆØ§ÛŒØ§"""
        gaps = []
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø²ÙˆØ§ÛŒØ§ÛŒ Ø±Ù‚Ø¨Ø§
        competitor_angles = {}
        
        for content_item in competitor_content:
            title = content_item.get('title', '')
            content = content_item.get('content', '')
            
            # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø²Ø§ÙˆÛŒÙ‡ Ø§Ø² Ø¹Ù†ÙˆØ§Ù† Ùˆ Ù…Ø­ØªÙˆØ§
            angle = self._identify_content_angle(title, content, language)
            
            if angle:
                if angle not in competitor_angles:
                    competitor_angles[angle] = []
                competitor_angles[angle].append(content_item)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø²ÙˆØ§ÛŒØ§ÛŒ Ø´Ù…Ø§
        your_angles = set()
        articles = site_content.get('articles', [])
        for article in articles:
            title = article.get('title', '')
            content = article.get('content', '')
            angle = self._identify_content_angle(title, content, language)
            if angle:
                your_angles.add(angle)
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø²ÙˆØ§ÛŒØ§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø±Ù‚Ø¨Ø§ Ø§Ù…Ø§ Ù†Ù‡ Ø¯Ø± Ø´Ù…Ø§
        for angle, content_items in competitor_angles.items():
            if angle not in your_angles:
                gaps.append({
                    'angle': angle,
                    'importance': len(content_items),
                    'competitor_count': len(content_items),
                    'content_items': content_items[:3],
                    'gap_type': 'angle'
                })
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ
        gaps.sort(key=lambda x: x['importance'], reverse=True)
        
        return gaps[:30]  # 30 gap Ø¨Ø±ØªØ±
    
    def _identify_content_angle(
        self,
        title: str,
        content: str,
        language: str
    ) -> Optional[str]:
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø²Ø§ÙˆÛŒÙ‡ Ù…Ø­ØªÙˆØ§"""
        text = f"{title} {content[:200]}".lower()
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø²Ø§ÙˆÛŒÙ‡
        angle_patterns = {
            'fa': {
                'how_to': ['Ú†Ú¯ÙˆÙ†Ù‡', 'Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ', 'Ø¢Ù…ÙˆØ²Ø´', 'Ù†Ø­ÙˆÙ‡'],
                'what_is': ['Ú†ÛŒØ³Øª', 'Ú†ÛŒØ³ØªØŸ', 'Ù…Ø¹Ù†ÛŒ', 'ØªØ¹Ø±ÛŒÙ'],
                'best': ['Ø¨Ù‡ØªØ±ÛŒÙ†', 'Ø¨Ø±ØªØ±ÛŒÙ†', 'Ø¹Ø§Ù„ÛŒ'],
                'comparison': ['Ù…Ù‚Ø§ÛŒØ³Ù‡', 'ØªÙØ§ÙˆØª', 'Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨ÛŒÙ†'],
                'review': ['Ù†Ù‚Ø¯', 'Ø¨Ø±Ø±Ø³ÛŒ', 'Ù†Ù‚Ø¯ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ'],
                'guide': ['Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ', 'Ú¯Ø§Ù… Ø¨Ù‡ Ú¯Ø§Ù…', 'Ù…Ø±Ø§Ø­Ù„'],
                'tips': ['Ù†Ú©Ø§Øª', 'ØªÙˆØµÛŒÙ‡', 'Ø±Ø§Ù‡Ú©Ø§Ø±'],
                'mistakes': ['Ø§Ø´ØªØ¨Ø§Ù‡Ø§Øª', 'Ø®Ø·Ø§Ù‡Ø§', 'Ù…Ø´Ú©Ù„Ø§Øª']
            },
            'en': {
                'how_to': ['how to', 'guide', 'tutorial', 'step by step'],
                'what_is': ['what is', 'definition', 'meaning'],
                'best': ['best', 'top', 'greatest'],
                'comparison': ['compare', 'vs', 'difference', 'versus'],
                'review': ['review', 'analysis'],
                'guide': ['guide', 'complete guide'],
                'tips': ['tips', 'tricks', 'advice'],
                'mistakes': ['mistakes', 'errors', 'common mistakes']
            }
        }
        
        patterns = angle_patterns.get(language, angle_patterns['en'])
        
        for angle_type, keywords in patterns.items():
            if any(keyword in text for keyword in keywords):
                return angle_type
        
        return None
    
    def _analyze_depth_gaps(
        self,
        site_content: Dict[str, Any],
        competitor_content: List[Dict[str, Any]],
        language: str
    ) -> List[Dict[str, Any]]:
        """ØªØ­Ù„ÛŒÙ„ ÙØ§ØµÙ„Ù‡ Ø¹Ù…Ù‚ Ù…Ø­ØªÙˆØ§"""
        gaps = []
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¹Ù…Ù‚ Ù…Ø­ØªÙˆØ§ÛŒ Ø´Ù…Ø§
        your_articles = site_content.get('articles', [])
        your_avg_depth = self._calculate_average_depth(your_articles)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¹Ù…Ù‚ Ù…Ø­ØªÙˆØ§ÛŒ Ø±Ù‚Ø¨Ø§
        competitor_avg_depth = self._calculate_average_depth(competitor_content)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙØ§ÙˆØª
        depth_difference = competitor_avg_depth - your_avg_depth
        
        if depth_difference > 0:
            # Ø±Ù‚Ø¨Ø§ Ø¹Ù…ÛŒÙ‚â€ŒØªØ± Ù‡Ø³ØªÙ†Ø¯
            gaps.append({
                'gap_type': 'depth',
                'your_average_depth': your_avg_depth,
                'competitor_average_depth': competitor_avg_depth,
                'difference': depth_difference,
                'recommendation': 'Ø§ÙØ²Ø§ÛŒØ´ Ø¹Ù…Ù‚ Ù…Ø­ØªÙˆØ§' if language == 'fa' else 'Increase content depth'
            })
        
        # ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù‚ Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ø´ØªØ±Ú©
        common_topics = self._find_common_topics(site_content, competitor_content)
        
        for topic in common_topics[:10]:  # 10 Ù…ÙˆØ¶ÙˆØ¹ Ø§ÙˆÙ„
            your_depth = self._calculate_topic_depth(topic, your_articles)
            competitor_depth = self._calculate_topic_depth(topic, competitor_content)
            
            if competitor_depth > your_depth * 1.2:  # 20% Ø¹Ù…ÛŒÙ‚â€ŒØªØ±
                gaps.append({
                    'gap_type': 'topic_depth',
                    'topic': topic,
                    'your_depth': your_depth,
                    'competitor_depth': competitor_depth,
                    'difference': competitor_depth - your_depth
                })
        
        return gaps
    
    def _calculate_average_depth(
        self,
        content_items: List[Dict[str, Any]]
    ) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¹Ù…Ù‚ Ù…Ø­ØªÙˆØ§"""
        if not content_items:
            return 0.0
        
        depths = []
        
        for item in content_items:
            # Ø¹Ù…Ù‚ Ø¨Ø± Ø§Ø³Ø§Ø³:
            # 1. Ø·ÙˆÙ„ Ù…Ø­ØªÙˆØ§ (40%)
            word_count = item.get('word_count', 0)
            word_score = min(word_count / 2000, 1.0) * 40
            
            # 2. ØªØ¹Ø¯Ø§Ø¯ headings (30%)
            headings = item.get('headings', [])
            heading_score = min(len(headings) / 10, 1.0) * 30
            
            # 3. ÙˆØ¬ÙˆØ¯ FAQ (15%)
            has_faq = item.get('has_faq', False)
            faq_score = 15 if has_faq else 0
            
            # 4. ÙˆØ¬ÙˆØ¯ ØªØµØ§ÙˆÛŒØ±/ÙˆÛŒØ¯ÛŒÙˆ (15%)
            has_media = item.get('has_images', False) or item.get('has_video', False)
            media_score = 15 if has_media else 0
            
            depth = word_score + heading_score + faq_score + media_score
            depths.append(depth)
        
        return sum(depths) / len(depths) if depths else 0.0
    
    def _calculate_topic_depth(
        self,
        topic: str,
        content_items: List[Dict[str, Any]]
    ) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¹Ù…Ù‚ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù…ÙˆØ¶ÙˆØ¹ Ø®Ø§Øµ"""
        topic_items = [
            item for item in content_items
            if topic.lower() in item.get('title', '').lower() or
               topic.lower() in item.get('content', '').lower()[:500]
        ]
        
        return self._calculate_average_depth(topic_items)
    
    def _find_common_topics(
        self,
        site_content: Dict[str, Any],
        competitor_content: List[Dict[str, Any]]
    ) -> List[str]:
        """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ø´ØªØ±Ú©"""
        your_topics = self._extract_topics(site_content, 'en')
        competitor_topics = set(self._extract_competitor_topics(competitor_content, 'en').keys())
        
        common = your_topics & competitor_topics
        return list(common)
    
    def _analyze_content_type_gaps(
        self,
        site_content: Dict[str, Any],
        competitor_content: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ØªØ­Ù„ÛŒÙ„ ÙØ§ØµÙ„Ù‡ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø­ØªÙˆØ§"""
        gaps = []
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø­ØªÙˆØ§ÛŒ Ø´Ù…Ø§
        your_content_types = set(site_content.get('content_types', []))
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø­ØªÙˆØ§ÛŒ Ø±Ù‚Ø¨Ø§
        competitor_content_types = set()
        for content_item in competitor_content:
            content_type = content_item.get('content_type', 'article')
            competitor_content_types.add(content_type)
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø§Ù†ÙˆØ§Ø¹ Ù…Ø­ØªÙˆØ§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø±Ù‚Ø¨Ø§ Ø§Ù…Ø§ Ù†Ù‡ Ø¯Ø± Ø´Ù…Ø§
        missing_types = competitor_content_types - your_content_types
        
        for content_type in missing_types:
            # Ø´Ù…Ø§Ø±Ø´ ØªØ¹Ø¯Ø§Ø¯ Ø§ÛŒÙ† Ù†ÙˆØ¹ Ù…Ø­ØªÙˆØ§ Ø¯Ø± Ø±Ù‚Ø¨Ø§
            count = sum(
                1 for item in competitor_content
                if item.get('content_type') == content_type
            )
            
            gaps.append({
                'content_type': content_type,
                'competitor_count': count,
                'your_count': 0,
                'gap_type': 'content_type',
                'importance': count
            })
        
        # ØªØ­Ù„ÛŒÙ„ ØªÙØ§ÙˆØª Ø¯Ø± ØªØ¹Ø¯Ø§Ø¯
        for content_type in your_content_types & competitor_content_types:
            your_count = sum(
                1 for item in site_content.get('articles', [])
                if item.get('content_type') == content_type
            )
            competitor_count = sum(
                1 for item in competitor_content
                if item.get('content_type') == content_type
            )
            
            if competitor_count > your_count * 1.5:  # 50% Ø¨ÛŒØ´ØªØ±
                gaps.append({
                    'content_type': content_type,
                    'competitor_count': competitor_count,
                    'your_count': your_count,
                    'gap_type': 'content_type_quantity',
                    'importance': competitor_count - your_count
                })
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ
        gaps.sort(key=lambda x: x['importance'], reverse=True)
        
        return gaps
    
    def _generate_recommendations(
        self,
        topic_gaps: List[Dict[str, Any]],
        angle_gaps: List[Dict[str, Any]],
        depth_gaps: List[Dict[str, Any]],
        content_type_gaps: List[Dict[str, Any]],
        language: str
    ) -> List[str]:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª"""
        recommendations = []
        
        if language == 'fa':
            # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ topic gaps
            if topic_gaps:
                high_importance = [gap for gap in topic_gaps if gap.get('importance', 0) >= 70]
                if high_importance:
                    recommendations.append(
                        f"âœ… {len(high_importance)} Ù…ÙˆØ¶ÙˆØ¹ Ø¨Ø§ Ø§Ù‡Ù…ÛŒØª Ø¨Ø§Ù„Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. "
                        f"Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ù…Ø­ØªÙˆØ§ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯."
                    )
                
                recommendations.append(
                    f"ğŸ“ {len(topic_gaps)} Ù…ÙˆØ¶ÙˆØ¹ Ú©Ù‡ Ø±Ù‚Ø¨Ø§ Ù¾ÙˆØ´Ø´ Ø¯Ø§Ø¯Ù‡â€ŒØ§Ù†Ø¯ Ø§Ù…Ø§ Ø´Ù…Ø§ Ù†Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯. "
                    f"Ø§ÛŒÙ† ÙØ±ØµØªâ€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø± Ø§ÙˆÙ„ÙˆÛŒØª Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯."
                )
            
            # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ angle gaps
            if angle_gaps:
                recommendations.append(
                    f"ğŸ¯ {len(angle_gaps)} Ø²Ø§ÙˆÛŒÙ‡ Ù…Ø®ØªÙ„Ù Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. "
                    f"Ø±ÙˆÛŒ Ø²ÙˆØ§ÛŒØ§ÛŒ Ù…Ø®ØªÙ„Ù Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ØªÙ…Ø±Ú©Ø² Ú©Ù†ÛŒØ¯."
                )
            
            # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ depth gaps
            if depth_gaps:
                depth_gap = next((g for g in depth_gaps if g.get('gap_type') == 'depth'), None)
                if depth_gap:
                    recommendations.append(
                        f"ğŸ“Š Ø¹Ù…Ù‚ Ù…Ø­ØªÙˆØ§ÛŒ Ø±Ù‚Ø¨Ø§ {depth_gap.get('difference', 0):.1f}% Ø¨ÛŒØ´ØªØ± Ø§Ø³Øª. "
                        f"Ù…Ø­ØªÙˆØ§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¹Ù…ÛŒÙ‚â€ŒØªØ± Ú©Ù†ÛŒØ¯."
                    )
            
            # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ content type gaps
            if content_type_gaps:
                missing_types = [g for g in content_type_gaps if g.get('your_count', 0) == 0]
                if missing_types:
                    types_str = ', '.join([g['content_type'] for g in missing_types[:3]])
                    recommendations.append(
                        f"ğŸ¨ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø­ØªÙˆØ§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø±Ù‚Ø¨Ø§ Ø§Ù…Ø§ Ù†Ù‡ Ø¯Ø± Ø´Ù…Ø§: {types_str}. "
                        f"Ø§ÛŒÙ† Ø§Ù†ÙˆØ§Ø¹ Ù…Ø­ØªÙˆØ§ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯."
                    )
        else:
            # English recommendations
            if topic_gaps:
                recommendations.append(
                    f"âœ… {len(topic_gaps)} topics identified that competitors cover but you don't. "
                    f"Prioritize creating content for these topics."
                )
            
            if angle_gaps:
                recommendations.append(
                    f"ğŸ¯ {len(angle_gaps)} different angles identified. "
                    f"Focus on different angles of topics."
                )
            
            if depth_gaps:
                depth_gap = next((g for g in depth_gaps if g.get('gap_type') == 'depth'), None)
                if depth_gap:
                    recommendations.append(
                        f"ğŸ“Š Competitor content is {depth_gap.get('difference', 0):.1f}% deeper. "
                        f"Increase your content depth."
                    )
        
        return recommendations
    
    def _calculate_summary(
        self,
        topic_gaps: List[Dict[str, Any]],
        angle_gaps: List[Dict[str, Any]],
        depth_gaps: List[Dict[str, Any]],
        content_type_gaps: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®Ù„Ø§ØµÙ‡"""
        high_importance_topics = [
            gap for gap in topic_gaps if gap.get('importance', 0) >= 70
        ]
        
        return {
            'total_topic_gaps': len(topic_gaps),
            'high_importance_topics': len(high_importance_topics),
            'total_angle_gaps': len(angle_gaps),
            'total_depth_gaps': len(depth_gaps),
            'total_content_type_gaps': len(content_type_gaps),
            'overall_gap_score': self._calculate_overall_gap_score(
                topic_gaps,
                angle_gaps,
                depth_gaps,
                content_type_gaps
            )
        }
    
    def _calculate_overall_gap_score(
        self,
        topic_gaps: List[Dict[str, Any]],
        angle_gaps: List[Dict[str, Any]],
        depth_gaps: List[Dict[str, Any]],
        content_type_gaps: List[Dict[str, Any]]
    ) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Overall Gap Score"""
        score = 0.0
        
        # ÙØ§Ú©ØªÙˆØ± 1: Topic Gaps (40%)
        if topic_gaps:
            avg_importance = sum(g.get('importance', 0) for g in topic_gaps) / len(topic_gaps)
            score += (avg_importance / 100) * 40
        else:
            score += 40  # Ø§Ú¯Ø± gap Ù†Ø¨Ø§Ø´Ø¯ØŒ Ø§Ù…ØªÛŒØ§Ø² Ú©Ø§Ù…Ù„
        
        # ÙØ§Ú©ØªÙˆØ± 2: Angle Gaps (25%)
        if angle_gaps:
            score += min(len(angle_gaps) / 10, 1.0) * 25
        else:
            score += 25
        
        # ÙØ§Ú©ØªÙˆØ± 3: Depth Gaps (20%)
        if depth_gaps:
            depth_gap = next((g for g in depth_gaps if g.get('gap_type') == 'depth'), None)
            if depth_gap:
                diff = depth_gap.get('difference', 0)
                score += max(0, 20 - (diff / 10))  # Ù‡Ø± 10 ÙˆØ§Ø­Ø¯ ØªÙØ§ÙˆØª = -1 Ø§Ù…ØªÛŒØ§Ø²
            else:
                score += 20
        else:
            score += 20
        
        # ÙØ§Ú©ØªÙˆØ± 4: Content Type Gaps (15%)
        if content_type_gaps:
            score += min(len(content_type_gaps) / 5, 1.0) * 15
        else:
            score += 15
        
        return min(score, 100.0)
    
    def _empty_gap_result(self) -> Dict[str, Any]:
        """Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù†ØªÛŒØ¬Ù‡ Ø®Ø§Ù„ÛŒ"""
        return {
            'topic_gaps': [],
            'angle_gaps': [],
            'depth_gaps': [],
            'content_type_gaps': [],
            'recommendations': [],
            'summary': {
                'total_topic_gaps': 0,
                'high_importance_topics': 0,
                'total_angle_gaps': 0,
                'total_depth_gaps': 0,
                'total_content_type_gaps': 0,
                'overall_gap_score': 0.0
            }
        }

